开发环境
----

+ Python 2.7
+ Requests

仅仅在Ubuntu 12.10上测试运行过，但windows也应该能正常执行。

思路
-----

本来打算借此机会使用Scrapy完成该任务。有2个原因放弃了选择

+ 公司无法看到Scrapy文档，所以选择了较为熟悉的Requests自己写。
+ Requests公司电脑上天然拥有。


思路参考之前写的豆瓣音乐爬虫，因为之前代码的失败【比如部分歌曲下载失败】的经验，所以此时一开始设计就打算多加入
诸如精细度的日志/进度信息等。因为网页和图片较多，所以多线程这里自然想到要线程池。对于需求提到的指定下载图片个数之后必须能结束下载，所以我加入了统计失败/成功的计数器。

具体流程如下：

         需求--->遇到过的相似问题【豆瓣音乐下载】-->Demo解决方案【先写一个单线程的递归遍历】--->改善适应本问题-->验收

另外需求提到复用性，比如可能会下载其他地方的，把下载处理图片的函数单独提取出来。这样下次需要改变website的时候尽可能最少改动代码。所以有了对于参数处理的opt_parser.py,爬图片所用的fetchurl.py


遇到的问题
----

+ 在打算将单线程爬取下载图片改为多线程的时候，发现其实线程池最好有一个是专门遍历页面的job队列和一个专门下载图片的job队列。这样做的好处是在图片下载这个线程里面可以单独分开加入任务。因为比较忙，所以线程池还是只设计了一个队列。这样的问题是我的代码效率和复用性都直线下滑。因为关于图片下载/url抓取的人物都放在了DownloadImageThread里面。一开始是一个线程循环下载图片，这样就是刚说的效率慢。所以我改成了判断url的特点来决定是下载还是遍历。于是这里内聚性/复用性都下降了。因为这样的任务本不属于它。
+ 单ip去遍历人家服务器。这是比较忌讳的。所以我在fetchurl.py里面设计为可以为其提供代理等参数。
+ 如何在下载完指定的图片数量后退出程序，【不直接exit()】,解决方案是计数器-可以参见progressinfoThread和DownloadImageThread的设计。
+ 时间问题。因为本来在实习，公司比较忙，只能在下午公司发布之后有空写。晚上学校又没电，所以时间严重不够。

想要优化的问题
-----

+ 完成任务队列的区分，下载图片是一个线程队列，url抓取也是一个队列。这样大概设计为3个主类。
  - 线程池   管理线程队列
  - 下载队列  此队列只负责下载可以延伸为数据持久化相关队列，比如可以直接插入redis/sqlite/文件等各种方式
  - 页面遍历队列 此队列只负责抓取出来想要处理的页面url,至于怎么出来它不应该关心。
+ 参数提示
 - 学git/hg 的help,想要支持子参数形式，对具体某个参数也能提供帮助，显然argparse是支持的，这也是我选择它不选择getopt的原因之一
 - 支持--log 日志控制/ --key只下载含有关键字的图片 --deep 爬取的深度
+ 写入数据库
 - 其实这只是改动下载队列的那个类就可以了。但是因为时间问题，现在耦合比较严重，难以做到重用性质
+ 分类下载，区分图片类别，人物等
+ 单元测试
+ Scrapy
+ .....

大概暂时就想到这些
----

